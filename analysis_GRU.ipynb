{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from warnings import filterwarnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, GRU, SpatialDropout1D, Dense\n",
    "from keras.layers import Embedding, Dropout, BatchNormalization, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "feature_train = pd.read_csv(r\"D:\\Tony\\Programming\\Datasets\\Yelp\\yelp_feature_train.csv\", encoding='utf-8')\n",
    "feature_val = pd.read_csv(r\"D:\\Tony\\Programming\\Datasets\\Yelp\\yelp_feature_val.csv\", encoding='utf-8')\n",
    "feature_test = pd.read_csv(r\"D:\\Tony\\Programming\\Datasets\\Yelp\\yelp_feature_test.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((428724, 18), (102135, 18), (129141, 18))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train.shape, feature_val.shape, feature_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_stars</th>\n",
       "      <th>review_like</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>restaurant_avg_stars</th>\n",
       "      <th>restaurant_review_count</th>\n",
       "      <th>city_review_count</th>\n",
       "      <th>centrality</th>\n",
       "      <th>user_avg_stars</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>user_fans</th>\n",
       "      <th>user_period</th>\n",
       "      <th>user_compliments</th>\n",
       "      <th>user_votes</th>\n",
       "      <th>popularity</th>\n",
       "      <th>pop_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>oj8qk3W7O9oq15VIs-vYJQ</td>\n",
       "      <td>UreiTV1I9i-XF6_bJhK6Iw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>new favorite place breakfast first time came h...</td>\n",
       "      <td>54</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>64</td>\n",
       "      <td>436.0</td>\n",
       "      <td>2.207263e-21</td>\n",
       "      <td>3.55</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>4.694428</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>YFqlMM1tFn0C4b9aHdzOgg</td>\n",
       "      <td>UreiTV1I9i-XF6_bJhK6Iw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>nice place food good nice wines pour decent si...</td>\n",
       "      <td>54</td>\n",
       "      <td>4.448276</td>\n",
       "      <td>29</td>\n",
       "      <td>839.0</td>\n",
       "      <td>2.207263e-21</td>\n",
       "      <td>3.55</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>4.367999</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>YFqlMM1tFn0C4b9aHdzOgg</td>\n",
       "      <td>UreiTV1I9i-XF6_bJhK6Iw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>nice place food good nice wines pour decent si...</td>\n",
       "      <td>42</td>\n",
       "      <td>4.448276</td>\n",
       "      <td>29</td>\n",
       "      <td>839.0</td>\n",
       "      <td>2.207263e-21</td>\n",
       "      <td>3.55</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>4.367999</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 user_id  review_stars  review_like  \\\n",
       "0  oj8qk3W7O9oq15VIs-vYJQ  UreiTV1I9i-XF6_bJhK6Iw           5.0            0   \n",
       "1  YFqlMM1tFn0C4b9aHdzOgg  UreiTV1I9i-XF6_bJhK6Iw           4.0            0   \n",
       "2  YFqlMM1tFn0C4b9aHdzOgg  UreiTV1I9i-XF6_bJhK6Iw           4.0            0   \n",
       "\n",
       "                                                text  text_len  \\\n",
       "0  new favorite place breakfast first time came h...        54   \n",
       "1  nice place food good nice wines pour decent si...        54   \n",
       "2  nice place food good nice wines pour decent si...        42   \n",
       "\n",
       "   restaurant_avg_stars  restaurant_review_count  city_review_count  \\\n",
       "0              4.375000                       64              436.0   \n",
       "1              4.448276                       29              839.0   \n",
       "2              4.448276                       29              839.0   \n",
       "\n",
       "     centrality  user_avg_stars  user_review_count  user_fans  user_period  \\\n",
       "0  2.207263e-21            3.55                104          0            8   \n",
       "1  2.207263e-21            3.55                104          0            8   \n",
       "2  2.207263e-21            3.55                104          0            8   \n",
       "\n",
       "   user_compliments  user_votes  popularity  pop_label  \n",
       "0                 2          80    4.694428        5.0  \n",
       "1                 2          80    4.367999        5.0  \n",
       "2                 2          80    4.367999        5.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select features\n",
    "train_num = feature_train.loc[:,~feature_train.columns.isin(['business_id', 'user_id', 'text', 'popularity', 'pop_label'])]\n",
    "val_num = feature_val.loc[:,~feature_val.columns.isin(['business_id', 'user_id', 'text', 'popularity', 'pop_label'])]\n",
    "test_num = feature_test.loc[:,~feature_test.columns.isin(['business_id', 'user_id', 'text', 'popularity', 'pop_label'])]\n",
    "\n",
    "# Fit scales\n",
    "train_num_scale = scaler.fit_transform(train_num)\n",
    "val_num_scale = scaler.transform(val_num)\n",
    "test_num_scale = scaler.transform(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((428724, 13), (102135, 13), (129141, 13))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num_scale.shape, val_num_scale.shape, test_num_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134152 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQ_LEN = 100\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(feature_train['text'])\n",
    "\n",
    "# Transform tokens into sequences\n",
    "sequences = tokenizer.texts_to_sequences(feature_train['text'])\n",
    "train_text_seq = pad_sequences(sequences, maxlen=MAX_SEQ_LEN)\n",
    "sequences = tokenizer.texts_to_sequences(feature_val['text'])\n",
    "val_text_seq = pad_sequences(sequences, maxlen=MAX_SEQ_LEN)\n",
    "sequences = tokenizer.texts_to_sequences(feature_test['text'])\n",
    "test_text_seq = pad_sequences(sequences, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pre-trained GloVe file\n",
    "embeddings_index = {}\n",
    "f = open(r'D:\\Tony\\Programming\\Datasets\\GloVe\\glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Link our index with GloVe index\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X-train and y-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "X_train = [train_text_seq, train_num_scale]\n",
    "y_train = pd.get_dummies(feature_train['pop_label'], drop_first=False).values\n",
    "\n",
    "# Val set\n",
    "X_val = [val_text_seq, val_num_scale]\n",
    "y_val = pd.get_dummies(feature_val['pop_label'], drop_first=False).values\n",
    "\n",
    "# Test set\n",
    "X_test = [test_text_seq, test_num_scale]\n",
    "y_test = pd.get_dummies(feature_test['pop_label'], drop_first=False).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: 1D CNN for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     13415300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 7)        2107        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 7)            0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           1344        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            520         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8)            32          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8)            0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5)            45          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,424,020\n",
      "Trainable params: 8,448\n",
      "Non-trainable params: 13,415,572\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRU model\n",
    "gru_input = Input((MAX_SEQ_LEN,))\n",
    "gru_embed = Embedding(input_dim=len(word_index)+1, output_dim=100, \n",
    "                      weights=[embedding_matrix], input_length=MAX_SEQ_LEN, trainable=False)(gru_input)\n",
    "gru_drop = SpatialDropout1D(0.2)(gru_embed)\n",
    "gru_main = GRU(10, dropout=0.2, recurrent_dropout=0.2)(gru_drop)\n",
    "\n",
    "\n",
    "# Other features\n",
    "other_input = Input((13,))\n",
    "\n",
    "# Concat models\n",
    "merged = Concatenate()([gru_main, other_input])\n",
    "merged_FC1 = Dense(64, activation='softplus')(merged)\n",
    "merged_norm1 = BatchNormalization()(merged_FC1)\n",
    "merged_drop1 = Dropout(0.2)(merged_norm1)\n",
    "merged_FC2 = Dense(64, activation='softplus')(merged_drop1)\n",
    "merged_norm2 = BatchNormalization()(merged_FC2)\n",
    "merged_drop2 = Dropout(0.2)(merged_norm2)\n",
    "merged_FC3 = Dense(8, activation='softplus')(merged_drop2)\n",
    "merged_norm3 = BatchNormalization()(merged_FC3)\n",
    "merged_drop3 = Dropout(0.2)(merged_norm3)\n",
    "merged_out = Dense(5, activation='softmax')(merged_drop3)\n",
    "concat_model = Model(inputs=[gru_input, other_input], outputs=merged_out)\n",
    "\n",
    "# Model summary\n",
    "concat_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 428724 samples, validate on 102135 samples\n",
      "Epoch 1/50\n",
      "258880/428724 [=================>............] - ETA: 22s - loss: 0.9214"
     ]
    }
   ],
   "source": [
    "filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "Adam = Adam(learning_rate=5e-4)\n",
    "\n",
    "# Compile model\n",
    "concat_model.compile(loss='categorical_crossentropy', optimizer=Adam)\n",
    "\n",
    "# Train model\n",
    "history = concat_model.fit(\n",
    "    X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "    validation_data=(X_val, y_val), verbose=1, shuffle=True, \n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://realpython.com/python-keras-text-classification/#a-primer-on-deep-neural-networks\n",
    "plt.style.use('ggplot')\n",
    "def plot_history(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate test predictions by restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for testing\n",
    "feature = feature_val\n",
    "X = X_val\n",
    "y = y_val\n",
    "\n",
    "# fit model on test set\n",
    "# if you use packages different from keras, please make sure that pred is n x 5 array\n",
    "pred = concat_model.predict(X)\n",
    "\n",
    "# pseudo-Gaussian product\n",
    "res_weight = pd.DataFrame(feature.iloc[:,0])\n",
    "res_weight['1'] = 1\n",
    "res_weight['2'] = 2\n",
    "res_weight['3'] = 3\n",
    "res_weight['4'] = 4\n",
    "res_weight['5'] = 5\n",
    "res_weight[['p1', 'p2', 'p3', 'p4', 'p5']] = pd.DataFrame(pred)\n",
    "res_weight['mean'] = res_weight['1']*res_weight['p1'] + \\\n",
    "                     res_weight['2']*res_weight['p2'] + \\\n",
    "                     res_weight['3']*res_weight['p3'] + \\\n",
    "                     res_weight['4']*res_weight['p4'] + \\\n",
    "                     res_weight['5']*res_weight['p5']\n",
    "res_weight['var'] = res_weight['p1']*((res_weight['1']-res_weight['mean'])**2) + \\\n",
    "                    res_weight['p2']*((res_weight['2']-res_weight['mean'])**2) + \\\n",
    "                    res_weight['p3']*((res_weight['3']-res_weight['mean'])**2) + \\\n",
    "                    res_weight['p4']*((res_weight['4']-res_weight['mean'])**2) + \\\n",
    "                    res_weight['p5']*((res_weight['5']-res_weight['mean'])**2)\n",
    "res_weight['repvar'] = 1/res_weight['var']\n",
    "res_weight['w_mean'] = res_weight['mean']*res_weight['repvar']\n",
    "\n",
    "# aggregate reviews by restaurants \n",
    "gb = res_weight.groupby(['business_id']).sum()\n",
    "gb['grand_mean'] = gb['w_mean']/gb['repvar']\n",
    "gb['grand_var'] = 1/gb['repvar']\n",
    "\n",
    "# aggregate true labels by restaurants\n",
    "y_true = pd.DataFrame(feature.iloc[:,[0]])\n",
    "y_true[['y_1', 'y_2', 'y_3', 'y_4', 'y_5']] = pd.DataFrame(y)\n",
    "y_gb = y_true.groupby(['business_id']).mean()\n",
    "\n",
    "# combine prediction and true labels for restaurants\n",
    "pred_mx = pd.merge(gb, y_gb, how='left', on=['business_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate RMSE\n",
    "true_labs = pred_mx[['y_1', 'y_2', 'y_3', 'y_4', 'y_5']].to_numpy().argmax(axis=1)+1\n",
    "pred_labs = np.round(pred_mx['grand_mean'])\n",
    "rmse = (mean_squared_error(true_labs, pred_labs))**(0.5)\n",
    "print(\"RMSE:\\t\", round(rmse, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "labels = [1,2,3,4,5]\n",
    "cm = confusion_matrix(true_labs, pred_labs, labels, normalize='true')\n",
    "cm_df = pd.DataFrame(cm, index=[i for i in labels], columns=[i for i in labels])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "ax = plt.subplot()\n",
    "sn.heatmap(cm_df, annot=True, cmap='BuPu', ax=ax)\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
